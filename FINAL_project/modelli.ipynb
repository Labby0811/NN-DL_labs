{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################################\n",
    "# Function to count the number of images per car make\n",
    "############################################################\n",
    "\n",
    "from collections import Counter\n",
    "def count_labels(dataset, make = True):\n",
    "\t# Extracts the labels from the dataset\n",
    "\tif make:\n",
    "\t\tlist = [make for _, make, _ in dataset]\n",
    "\telse:\n",
    "\t\tlist = [model for _, _, model in dataset]\n",
    "\n",
    "\t# Counts the occurrences of each label\n",
    "\tdataset_count = Counter(list)\n",
    "\n",
    "\t# Transform the labels into the string using get_names function\n",
    "\tif make:\n",
    "\t\tdataset_count_strings = {tuple(get_names(mat_file, [label, 0]))[0]: count for label, count in dataset_count.items()}\n",
    "\telse:\n",
    "\t\tdataset_count_strings = {tuple(get_names(mat_file, [0, label]))[1]: count for label, count in dataset_count.items()}\n",
    "\n",
    "\t# Print the labels and their counts\n",
    "\tprint(\"Dataset labels:\", dataset_count_strings)\n",
    "\n",
    "\t# Sort the labels by alphabetical order\n",
    "\tsorted_labels = sorted(dataset_count_strings.items(), key=lambda x: x[0])\n",
    "\tlabels, counts = zip(*sorted_labels)\n",
    "\n",
    "\t# Create the plot\n",
    "\tplt.figure(figsize=(15, 5))\n",
    "\tplt.bar(labels, counts)\n",
    "\tplt.xticks(rotation=90)\n",
    "\tplt.ylabel(\"Number of images\")\n",
    "\tplt.title(\"Number of images per car make in the training dataset\")\n",
    "\tplt.show()\n",
    "\n",
    "\treturn dataset_count_strings\n",
    "\n",
    "\n",
    "\n",
    "train_counts = count_labels(train_dataset)\n",
    "val_counts = count_labels(val_dataset)\n",
    "test_counts = count_labels(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#HERE I TRY USING THE GITHUB LINK TECHNIQUE\n",
    "from torchvision.models import Inception_V3_Weights\n",
    "# Load the model\n",
    "net = torchvision.models.inception_v3(weights=Inception_V3_Weights.IMAGENET1K_V1)\n",
    "\n",
    "# Change the number of output features\n",
    "MODELS_NUM = 431\n",
    "MAKE_NUM = 163\n",
    "\n",
    "num_features = net.fc.in_features\n",
    "net.fc = nnpo.Linear(num_features, MAKE_NUM)\n",
    "\n",
    "if torch.cuda.device_count() > 1:\n",
    "    net = torch.nn.DataParallel(net)\n",
    "\n",
    "net.cuda()\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "criterion.cuda()\n",
    "\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.01, momentum=0.9)\n",
    "my_scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)\n",
    "dloaders = {'train': train_loader, 'valid': val_loader}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "\n",
    "def train_model(dataloders, model, criterion, optimizer, scheduler, num_epochs=10000):\n",
    "    logger = SummaryWriter()\n",
    "    since = time.time()\n",
    "    best_acc = 0.0\n",
    "    dataset_sizes = {'train': len(dataloders['train'].dataset),\n",
    "                     'valid': len(dataloders['valid'].dataset)}\n",
    "    print(\"train and valid sizes %d %d\" % (len(dataloders['train'].dataset), len(dataloders['valid'].dataset)))\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        epoc_time = time.time()\n",
    "        print(f'Epoch {epoch}/{num_epochs - 1}')\n",
    "        print('-' * 10)\n",
    "\n",
    "        for phase in ['train', 'valid']:\n",
    "            if phase == 'train':\n",
    "                scheduler.step()\n",
    "                model.train()\n",
    "            else:\n",
    "                model.eval()\n",
    "\n",
    "            running_loss = 0.0\n",
    "\n",
    "            if phase == 'train':\n",
    "                train_running_corrects = 0\n",
    "            else:\n",
    "                valid_running_corrects = 0\n",
    "\n",
    "            # Use tqdm to create a progress bar\n",
    "            with tqdm(total=len(dataloders[phase]), desc=f'{phase} Epoch {epoch + 1}/{num_epochs}', unit='batch') as pbar:\n",
    "                for inputs, label_make, label_model in dataloders[phase]:\n",
    "                    t1 = time.time()\n",
    "                    labels = label_make\n",
    "                    inputs, labels = inputs.cuda(), labels.cuda()\n",
    "\n",
    "                    optimizer.zero_grad()\n",
    "\n",
    "                    t2 = time.time()\n",
    "                    outputs = model(inputs)\n",
    "                    t3 = time.time()\n",
    "\n",
    "                    if phase == 'train':\n",
    "                        loss1 = criterion(outputs[0], labels)\n",
    "                        loss2 = criterion(outputs[1], labels)\n",
    "                        loss = loss1 + loss2  # Future work, change to e.g. loss1 + 0.4 x loss2\n",
    "                        preds = torch.max(outputs[0], 1)[1]\n",
    "                        train_running_corrects += torch.sum(preds == labels.data)\n",
    "                    else:\n",
    "                        loss = criterion(outputs, labels)\n",
    "                        _, preds = torch.max(outputs.data, 1)\n",
    "                        valid_running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "                    t4 = time.time()\n",
    "\n",
    "                    running_loss += loss.item()\n",
    "                    pbar.update(1)  # Update the progress bar\n",
    "\n",
    "            if phase == 'train':\n",
    "                train_epoch_loss = running_loss / dataset_sizes[phase]\n",
    "                train_epoch_acc = train_running_corrects.double() / dataset_sizes[phase] * 100\n",
    "            else:\n",
    "                valid_epoch_loss = running_loss / dataset_sizes[phase]\n",
    "                valid_epoch_acc = valid_running_corrects.double() / dataset_sizes[phase] * 100\n",
    "\n",
    "            if phase == 'valid' and valid_epoch_acc > best_acc:\n",
    "                best_acc = valid_epoch_acc\n",
    "                best_model_wts = model.state_dict()\n",
    "                torch.save({\n",
    "                    'epoch': epoch,\n",
    "                    'epochTrainLoss': train_epoch_loss,\n",
    "                    'epochValidLoss': valid_epoch_loss,\n",
    "                    'epochTrainAcc': train_epoch_acc,\n",
    "                    'epochValidAcc': valid_epoch_acc,\n",
    "                    'model_state_dict': model.state_dict(),\n",
    "                    'optimizer_state_dict': optimizer.state_dict(),\n",
    "                }, os.path.join(DATA_PATH, \"Inception_classification.pth\"))\n",
    "\n",
    "        epocTotalTime = time.time() - epoc_time\n",
    "        epocLoadDataTime = t2 - t1\n",
    "        epocForwardTime = t3 - t2\n",
    "        epocLossBackwardTime = t4 - t3\n",
    "        print('Epoch [{}/{}] train loss: {:.4f} acc: {:.4f}% '\n",
    "              'valid loss: {:.4f} acc: {:.4f}% Time: {:.0f}s train corr: {:d}  valid corr: {:d}  '.format(\n",
    "                epoch, num_epochs - 1,\n",
    "                train_epoch_loss, train_epoch_acc,\n",
    "                valid_epoch_loss, valid_epoch_acc,\n",
    "                (time.time() - epoc_time),\n",
    "                train_running_corrects,\n",
    "                valid_running_corrects\n",
    "              ))\n",
    "        logger.add_scalar('Train/epocLoss', train_epoch_loss, epoch)\n",
    "        logger.add_scalar('Train/accuracy', train_epoch_acc, epoch)\n",
    "        logger.add_scalar('Valid/epocLoss', valid_epoch_loss, epoch)\n",
    "        logger.add_scalar('Valid/accuracy', valid_epoch_acc, epoch)\n",
    "        logger.add_scalar('Train/epocTotalTime', epocTotalTime, epoch)\n",
    "        logger.add_scalar('Train/epocLoadDataTime', epocLoadDataTime, epoch)\n",
    "        logger.add_scalar('Train/epocForwardTime', epocForwardTime, epoch)\n",
    "        logger.add_scalar('Train/epocLossBackward', epocLossBackwardTime, epoch)\n",
    "\n",
    "    print('Best val Acc: {:4f}'.format(best_acc))\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epoches = 10000\n",
    "start_time = time.time()\n",
    "\n",
    "model = train_model(dloaders, net, criterion, optimizer, my_scheduler, num_epochs=num_epoches)\n",
    "print('Training time: {:10f} minutes'.format((time.time()-start_time)/60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# toy exemple to understand the cross entropy loss\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Supponiamo di avere un batch di dimensione 3 e 5 classi\n",
    "logits = torch.tensor([[0.0, 2.0, 0.0, 4.0, 1.0],\n",
    "                       [1.0, 0.0, 0.0, 1.0, 4.0],\n",
    "                       [4.0, 1.0, 2.0, 0.0, 1.0]], requires_grad=True)\n",
    "\n",
    "# Etichette di classe vere per ciascun esempio nel batch\n",
    "labels = torch.tensor([3, 4, 0])\n",
    "\n",
    "print(logits[0], logits[1], logits[2])  # Stampa i logit per ciascun batch\n",
    "\n",
    "# Definisci la funzione di perdita\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Calcola la perdita\n",
    "loss = criterion(logits, labels)\n",
    "\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PRETRAINED NET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODELS_NUM = 431\n",
    "MAKE_NUM = 163\n",
    "car_make_bool = True   ## TODO Remove this line\n",
    "checkpoint_name = \"checkpoint.pth.tar\"\n",
    "checkpoint_path = os.path.join(DATA_PATH, checkpoint_name)\n",
    "\n",
    "# Function to save the checkpoint\n",
    "def save_checkpoint(state, filename):\n",
    "    torch.save(state, filename)\n",
    "\n",
    "# Function to load the checkpoint\n",
    "def load_checkpoint(model, optimizer, scheduler, filename):\n",
    "    if os.path.isfile(filename):\n",
    "        print(f\"Loading checkpoint '{filename}'\")\n",
    "        checkpoint = torch.load(filename)\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
    "        epoch = checkpoint['epoch']\n",
    "        best_acc = checkpoint['best_acc']\n",
    "        print(f\"Loaded checkpoint '{filename}' (epoch {epoch})\")\n",
    "        return model, optimizer, scheduler, epoch, best_acc\n",
    "    else:\n",
    "        print(f\"No checkpoint found at '{filename}'\")\n",
    "        return model, optimizer, scheduler, 0, 0.0\n",
    "\n",
    "\n",
    "# Function to calculate the accuracy of the model\n",
    "def accuracy(outputs, labels):\n",
    "\t_, preds = torch.max(outputs, dim=1)   # Get the prediction label for the batch\n",
    "\treturn torch.tensor(torch.sum(preds == labels).item() / len(preds))  #sees how many predictions are correct in the batch\n",
    "\n",
    "####TODO Check if the function is correct\n",
    "# Function to predict the class of an image\n",
    "def predict_image(image_path, model, transform):\n",
    "\timage = Image.open(image_path).convert(\"RGB\")\n",
    "\timage = transform(image).unsqueeze(0).to(device)\n",
    "\toutputs = model(image)\n",
    "\t_, preds = torch.max(outputs, 1)\n",
    "\treturn preds.item()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Function to train the model\n",
    "def train_model(model, train_loader, val_loader, criterion, optimizer, scheduler, num_epochs=25, start_epoch=0, best_acc=0.0):\n",
    "    best_model_wts = None\n",
    "\n",
    "    for epoch in range(start_epoch, start_epoch + num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch, start_epoch + num_epochs - 1))\n",
    "        print('-' * 10)\n",
    "        # if epoch > 20:\n",
    "        #     # Unfreeze the layers after 20 epochs\n",
    "        #     for param in model.parameters():\n",
    "        #         param.requires_grad = True\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train()  # Set model to training mode\n",
    "                dataloader = train_loader\n",
    "            else:\n",
    "                model.eval()   # Set model to evaluation mode\n",
    "                dataloader = val_loader\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            # Iterate over data\n",
    "            iterator = tqdm(dataloader, desc=phase)\n",
    "            for batch_x, batch_car_make, batch_car_model in iterator:\n",
    "                batch_x = batch_x.to(device)\n",
    "                if car_make_bool:\n",
    "                    labels = batch_car_make.to(device)\n",
    "                else:\n",
    "                    labels = batch_car_model.to(device)\n",
    "\n",
    "                # Zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # Forward pass\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = model(batch_x)   # batch size x num_classes\n",
    "                    loss = criterion(outputs, labels)\n",
    "\n",
    "                    # Backward pass + optimize only if in training phase\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                # Statistics\n",
    "                running_loss += loss.item() * batch_x.size(0)  #type float like loss.detach().cpu().numpy()\n",
    "                running_corrects += accuracy(outputs, labels)\n",
    "\n",
    "            if phase == 'train':\n",
    "                train_loss = running_loss / len(train_dataset)\n",
    "                train_acc = running_corrects.double() / len(train_dataset)\n",
    "            else:\n",
    "                val_loss = running_loss / len(val_dataset)\n",
    "                val_acc = running_corrects.double() / len(val_dataset)\n",
    "\n",
    "                # Update the learning rate\n",
    "                scheduler.step()\n",
    "\n",
    "                # Save the best model\n",
    "                if val_acc > best_acc:\n",
    "                    best_acc = val_acc\n",
    "                    best_model_wts = model.state_dict()\n",
    "                    # Save the model only if it is the best model\n",
    "                    save_checkpoint({\n",
    "                        'epoch': epoch,\n",
    "                        'model_state_dict': model.state_dict(),\n",
    "                        'best_model_wts': best_model_wts,\n",
    "                        'optimizer_state_dict': optimizer.state_dict(),\n",
    "                        'scheduler_state_dict': scheduler.state_dict(),\n",
    "                        'best_acc': best_acc,\n",
    "                        'train_loss': train_loss,\n",
    "                        'val_loss': val_loss,\n",
    "                        'train_acc': train_acc,\n",
    "                        'val_acc': val_acc,\n",
    "                    }, checkpoint_path)\n",
    "                    print(\"Model saved\")\n",
    "\n",
    "            print(f'{phase} Loss: {train_loss if phase == \"train\" else val_loss:.4f} Acc: {train_acc if phase == \"train\" else val_acc:.4f}')\n",
    "\n",
    "        print()\n",
    "\n",
    "    print('Best val Acc: {:4f}'.format(best_acc))\n",
    "\n",
    "    # Load the best model weights\n",
    "    model, optimizer, scheduler, start_epoch, best_acc = load_checkpoint(model, optimizer, scheduler, checkpoint_path)\n",
    "\n",
    "    return model\n",
    "\n",
    "# Function to evaluate the model\n",
    "def evaluate_model(model, dataloader):\n",
    "\tmodel.eval()\n",
    "\trunning_loss = 0.0\n",
    "\trunning_corrects = 0\n",
    "\n",
    "\titerator = tqdm(dataloader, desc=\"Evaluation\")\n",
    "\t# Iterate over data\n",
    "\tfor batch_x, batch_car_make, batch_car_model in iterator:\n",
    "\t\tbatch_x = batch_x.to(device)\n",
    "\t\tif car_make_bool:\n",
    "\t\t\tlabels = batch_car_make.to(device)\n",
    "\t\telse:\n",
    "\t\t\tlabels = batch_car_model.to(device)\n",
    "\n",
    "\t\t# Forward pass\n",
    "\t\twith torch.no_grad():\n",
    "\t\t\toutputs = model(batch_x)\n",
    "\t\t\tloss = criterion(outputs, labels)\n",
    "\n",
    "\t\t# Statistics\n",
    "\trunning_loss += loss.item() * batch_x.size(0)\n",
    "\trunning_corrects += accuracy(outputs, labels)\n",
    "\n",
    "\tepoch_loss = running_loss / len(dataloader.dataset)\n",
    "\tepoch_acc = running_corrects.double() / len(dataloader.dataset)\n",
    "\n",
    "\tprint('Loss: {:.4f} Acc: {:.4f}'.format(epoch_loss, epoch_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.models as models\n",
    "from torchvision.models import ResNet50_Weights\n",
    "\n",
    "# Upload a pre-trained model to ImageNet\n",
    "model = models.resnet50(weights=ResNet50_Weights.IMAGENET1K_V2)\n",
    "#model =  models.resnet50()   #NOT pretrained\n",
    "\n",
    "# # Freeze all the layers in the network\n",
    "# for param in model.parameters():\n",
    "# \tparam.requires_grad = False\n",
    "\n",
    "# Get the number of features in the model\n",
    "num_features = model.fc.in_features\n",
    "print(\"Number of features:\", num_features)\n",
    "# Replace the final layer with a fully connected layer with the number of features in the model\n",
    "if car_make_bool:\n",
    "\tmodel.fc = nnpo.Linear(num_features, MAKE_NUM)\n",
    "else:\n",
    "\tmodel.fc = nnpo.Linear(num_features, MODELS_NUM)\n",
    "\n",
    "# Move the model to the device\n",
    "model = model.to(device)\n",
    "\n",
    "# Define the loss function\n",
    "criterion = nnpo.CrossEntropyLoss()\n",
    "criterion = criterion.to(device)\n",
    "\n",
    "# Define the optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Define the learning rate scheduler\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)\n",
    "\n",
    "# Load the checkpoint\n",
    "model, optimizer, scheduler, start_epoch, best_acc = load_checkpoint(model, optimizer, scheduler, checkpoint_path)\n",
    "print(\"Starting epoch:\", start_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "model = train_model(model, train_loader, val_loader, criterion, optimizer, scheduler, num_epochs=12, start_epoch=start_epoch, best_acc=best_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model on the test set\n",
    "evaluate_model(model, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################\n",
    "# load previous save model\n",
    "###############################\n",
    "\n",
    "# Load the model\n",
    "model, optimizer, scheduler, start_epoch, best_acc = load_checkpoint(model, optimizer, scheduler, checkpoint_path)\n",
    "model.eval()\n",
    "\n",
    "# Predict the class of an image\n",
    "image_path = os.path.join(test_dir, )\n",
    "\n",
    "# Load the image\n",
    "random_number = random.randint(0, len(test_dataset))\n",
    "img, car_make, car_model = test_dataset[random_number]\n",
    "img = denormalize(img, mean, std)\n",
    "imshow(img)\n",
    "print(\"Make:\",car_make)\n",
    "print(\"Model:\",car_model)\n",
    "\n",
    "print(get_names(mat_file, [car_make, car_model]))\n",
    "\n",
    "# Predict the class of the image\n",
    "pred = model(img.unsqueeze(0).to(device)).argmax().item()\n",
    "print(\"Prediction:\", pred, get_names(mat_file, [pred, 0]))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
